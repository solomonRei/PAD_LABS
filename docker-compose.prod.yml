version: '3.8'

services:
  usersvc-postgres:
    image: postgres:15-alpine
    container_name: usersvc-postgres
    environment:
      POSTGRES_DB: ${USER_SVC_DB_NAME}
      POSTGRES_USER: ${USER_SVC_DB_USER}
      POSTGRES_PASSWORD: ${USER_SVC_DB_PASSWORD}
      PGDATA: /var/lib/postgresql/data
    volumes:
      - usersvc_postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"   # usersvc Postgres
    networks:
      - pad-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${USER_SVC_DB_USER} -d ${USER_SVC_DB_NAME}"]
      interval: 10s
      timeout: 5s
      retries: 5

  user-service:
    image: laineer/pad-user-svc:latest
    container_name: pad-user-service
    environment:
      SPRING_PROFILES_ACTIVE: prod
      SERVER_PORT: ${USER_SVC_SERVER_PORT:-8080}
      USER_SVC_SPRING_DATASOURCE_URL: jdbc:postgresql://usersvc-postgres:5432/${USER_SVC_DB_NAME}
      USER_SVC_SPRING_DATASOURCE_USERNAME: ${USER_SVC_DB_USERNAME}
      USER_SVC_SPRING_DATASOURCE_PASSWORD: ${USER_SVC_DB_PASSWORD}
      USER_SVC_DISCORD_CLIENT_ID: ${USER_SVC_DISCORD_CLIENT_ID}
      USER_SVC_DISCORD_CLIENT_SECRET: ${USER_SVC_DISCORD_CLIENT_SECRET}
      USER_SVC_DISCORD_BOT_TOKEN: ${USER_SVC_DISCORD_BOT_TOKEN}
      USER_SVC_FAF_GUILD_ID: ${USER_SVC_FAF_GUILD_ID}
      USER_SVC_JWT_SECRET: ${USER_SVC_JWT_SECRET}
      USER_SVC_JWT_TTL_MINUTES: ${USER_SVC_JWT_TTL_MINUTES}
      USER_SVC_SPRING_JPA_HIBERNATE_DDL_AUTO: ${USER_SVC_SPRING_JPA_HIBERNATE_DDL_AUTO}
      USER_SVC_APP_DATA_INITIALIZE: ${USER_SVC_APP_DATA_INITIALIZE:-true}
      LOGSTASH_HOST: logstash
      LOGSTASH_PORT: 5000
      LOGSTASH_LEVEL: INFO
      EUREKA_SERVER_URL: http://discovery-server:8761/eureka/
      EUREKA_REGISTER: true
      EUREKA_FETCH: true
      EUREKA_ENABLED: true
      MANAGEMENT_ENDPOINTS_WEB_EXPOSURE_INCLUDE: health,info,prometheus
      MANAGEMENT_METRICS_EXPORT_PROMETHEUS_ENABLED: true
    ports:
      - "${USER_SVC_EXTERNAL_PORT:-8080}:${USER_SVC_SERVER_PORT:-8080}"
    networks:
      - pad-network
    depends_on:
      usersvc-postgres:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${USER_SVC_SERVER_PORT}/userservice/api/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  gateway-redis:
    image: redis:7-alpine
    container_name: pad-gateway-redis
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
    volumes:
      - gateway_redis_data:/data
    networks:
      - pad-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  gateway-service:
    image: smeloved/pad-gateway-svc:latest
    container_name: pad-gateway-service
    environment:
      SPRING_PROFILES_ACTIVE: prod
      GATEWAY_SVC_SERVER_PORT: 9090
      SPRING_REDIS_HOST: gateway-redis
      SPRING_REDIS_PORT: 6379
      GATEWAY_SVC_SPRING_DATASOURCE_URL: jdbc:postgresql://usersvc-postgres:5432/${USER_SVC_DB_NAME}
      GATEWAY_SVC_SPRING_DATASOURCE_USERNAME: ${USER_SVC_DB_USERNAME}
      GATEWAY_SVC_SPRING_DATASOURCE_PASSWORD: ${USER_SVC_DB_PASSWORD}
      DISCORD_CLIENT_ID: ${USER_SVC_DISCORD_CLIENT_ID}
      DISCORD_CLIENT_SECRET: ${USER_SVC_DISCORD_CLIENT_SECRET}
      DISCORD_BOT_TOKEN: ${USER_SVC_DISCORD_BOT_TOKEN}
      DISCORD_GUILD_ID: ${USER_SVC_FAF_GUILD_ID}
      JWT_SECRET: ${USER_SVC_JWT_SECRET}
      JWT_TTL_MINUTES: ${USER_SVC_JWT_TTL_MINUTES:-120}
      CORS_ALLOWED_ORIGINS: ${GATEWAY_CORS_ALLOWED_ORIGINS:-http://localhost:3000,https://yourdomain.com}
      GATEWAY_TIMEOUT_MS: ${GATEWAY_TIMEOUT_MS:-30000}
      GATEWAY_MAX_CONCURRENT_REQUESTS: ${GATEWAY_MAX_CONCURRENT_REQUESTS:-100}
      CACHE_TTL_MINUTES: ${GATEWAY_CACHE_TTL_MINUTES:-10}
      # Downstream Services URLs (Docker internal network)
      USER_SERVICE_URL: http://user-service:8080
      NOTIFICATION_SERVICE_URL: http://notification-service:8080
      COMMUNICATION_SERVICE_URL: http://communication-service:8085
      LOSTFOUND_SERVICE_URL: http://lost-and-found-service:3000
      FUNDRAISING_SERVICE_URL: http://fundraising-service:8000
      SHARING_SERVICE_URL: http://sharing-service:8001
      BUDGETING_SERVICE_URL: http://budgeting-service:3001
      CABBOOKING_SERVICE_URL: http://cab-booking-service:8000
      CHECKIN_SERVICE_URL: http://check-in-service:8012
      TEA_SERVICE_URL: http://tea-service:8084
      LOGSTASH_HOST: logstash
      LOGSTASH_PORT: 5000
      LOGSTASH_LEVEL: INFO
      EUREKA_SERVER_URL: http://discovery-server:8761/eureka/
      EUREKA_REGISTER: true
      EUREKA_FETCH: true
      EUREKA_ENABLED: true
      MANAGEMENT_ENDPOINTS_WEB_EXPOSURE_INCLUDE: health,info,prometheus
      MANAGEMENT_METRICS_EXPORT_PROMETHEUS_ENABLED: true
    ports:
      - "9090:9090"
    networks:
      - pad-network
    depends_on:
      gateway-redis:
        condition: service_healthy
      usersvc-postgres:
        condition: service_healthy
      user-service:
        condition: service_started
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9090/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  notification-service:
    image: laineer/pad-notification-svc:latest
    container_name: pad-notification-service
    environment:
      SPRING_PROFILES_ACTIVE: prod
      SERVER_PORT: ${NOTIFICATION_SVC_SERVER_PORT}
      NOTIFICATION_SVC_MAIL_ENABLED: ${NOTIFICATION_SVC_MAIL_ENABLED:-false}
      NOTIFICATION_SVC_MAIL_FROM: ${NOTIFICATION_SVC_MAIL_FROM:-no-reply@example.com}
      NOTIFICATION_SVC_MAIL_USERNAME: ${NOTIFICATION_SVC_MAIL_USERNAME:-test}
      NOTIFICATION_SVC_MAIL_PASSWORD: ${NOTIFICATION_SVC_MAIL_PASSWORD:-test}
      NOTIFICATION_SVC_MONGO_URI: ${NOTIFICATION_SVC_MONGO_URI}
      NOTIFICATION_SVC_MONGO_DATABASE: ${NOTIFICATION_SVC_MONGO_DATABASE}
      LOGSTASH_HOST: logstash
      LOGSTASH_PORT: 5000
      LOGSTASH_LEVEL: INFO
      EUREKA_SERVER_URL: http://discovery-server:8761/eureka/
      EUREKA_REGISTER: true
      EUREKA_FETCH: true
      EUREKA_ENABLED: true
      MANAGEMENT_ENDPOINTS_WEB_EXPOSURE_INCLUDE: health,info,prometheus
      MANAGEMENT_METRICS_EXPORT_PROMETHEUS_ENABLED: true
    ports:
      - "8081:8080"
    networks:
      - pad-network
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:${NOTIFICATION_SVC_SERVER_PORT:-8080}/notificationsvc/api/v1/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  notification-service-2:
    image: laineer/pad-notification-svc:latest
    container_name: pad-notification-service-2
    environment:
      SPRING_PROFILES_ACTIVE: prod
      SERVER_PORT: ${NOTIFICATION_SVC_SERVER_PORT}
      NOTIFICATION_SVC_MAIL_ENABLED: ${NOTIFICATION_SVC_MAIL_ENABLED:-false}
      NOTIFICATION_SVC_MAIL_FROM: ${NOTIFICATION_SVC_MAIL_FROM:-no-reply@example.com}
      NOTIFICATION_SVC_MAIL_USERNAME: ${NOTIFICATION_SVC_MAIL_USERNAME:-test}
      NOTIFICATION_SVC_MAIL_PASSWORD: ${NOTIFICATION_SVC_MAIL_PASSWORD:-test}
      NOTIFICATION_SVC_MONGO_URI: ${NOTIFICATION_SVC_MONGO_URI}
      NOTIFICATION_SVC_MONGO_DATABASE: ${NOTIFICATION_SVC_MONGO_DATABASE}
      LOGSTASH_HOST: logstash
      LOGSTASH_PORT: 5000
      LOGSTASH_LEVEL: INFO
      EUREKA_SERVER_URL: http://discovery-server:8761/eureka/
      EUREKA_REGISTER: true
      EUREKA_FETCH: true
      EUREKA_ENABLED: true
      MANAGEMENT_ENDPOINTS_WEB_EXPOSURE_INCLUDE: health,info,prometheus
      MANAGEMENT_METRICS_EXPORT_PROMETHEUS_ENABLED: true
    ports:
      - "8071:8080"
    networks:
      - pad-network
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:${NOTIFICATION_SVC_SERVER_PORT:-8080}/notificationsvc/api/v1/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  tea_service_postgres:
    container_name: tea_service_postgres
    image: postgres:16
    environment:
      POSTGRES_DB: ${TEA_SVC_DB_NAME}
      POSTGRES_USER: ${TEA_SVC_SPRING_DATASOURCE_USERNAME}
      POSTGRES_PASSWORD: ${TEA_SVC_SPRING_DATASOURCE_PASSWORD}
      PGDATA: /var/lib/postgresql/data
    volumes:
      - teasvc_postgres_data:/var/lib/postgresql/data
    ports:
      - "5433:5432"
    command: -p 5432
    restart: always
    networks:
      - pad-network
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U ${TEA_SVC_SPRING_DATASOURCE_USERNAME} -d ${TEA_SVC_DB_NAME}" ]
      interval: 10s
      timeout: 5s
      retries: 5

  tea-service:
    image: smeloved/pad-tea-svc:latest
    container_name: pad-tea-service
    environment:
      SPRING_PROFILES_ACTIVE: prod
      TEA_SVC_SERVER_PORT: ${TEA_SVC_SERVER_PORT:-8084}
      TEA_SVC_SPRING_DATASOURCE_URL: jdbc:postgresql://tea_service_postgres:5432/${TEA_SVC_DB_NAME}
      TEA_SVC_SPRING_DATASOURCE_USERNAME: ${TEA_SVC_SPRING_DATASOURCE_USERNAME}
      TEA_SVC_SPRING_DATASOURCE_PASSWORD: ${TEA_SVC_SPRING_DATASOURCE_PASSWORD}
      TEA_SVC_SPRING_JPA_HIBERNATE_DDL_AUTO: ${TEA_SVC_SPRING_JPA_HIBERNATE_DDL_AUTO:-validate}
      TEA_SVC_DATA_SEEDER_RUN: ${TEA_SVC_DATA_SEEDER_RUN:-false}
      LOGSTASH_HOST: logstash
      LOGSTASH_PORT: 5000
      LOGSTASH_LEVEL: INFO
      EUREKA_SERVER_URL: http://discovery-server:8761/eureka/
      EUREKA_REGISTER: true
      EUREKA_FETCH: true
      EUREKA_ENABLED: true
      MANAGEMENT_ENDPOINTS_WEB_EXPOSURE_INCLUDE: health,info,prometheus
      MANAGEMENT_METRICS_EXPORT_PROMETHEUS_ENABLED: true
    ports:
      - "8082:8084"
    networks:
      - pad-network
    depends_on:
      tea_service_postgres:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${TEA_SVC_SERVER_PORT:-8084}/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  tea-service-2:
    image: smeloved/pad-tea-svc:latest
    container_name: pad-tea-service-2
    environment:
      SPRING_PROFILES_ACTIVE: prod
      TEA_SVC_SERVER_PORT: ${TEA_SVC_SERVER_PORT:-8084}
      TEA_SVC_SPRING_DATASOURCE_URL: jdbc:postgresql://tea_service_postgres:5432/${TEA_SVC_DB_NAME}
      TEA_SVC_SPRING_DATASOURCE_USERNAME: ${TEA_SVC_SPRING_DATASOURCE_USERNAME}
      TEA_SVC_SPRING_DATASOURCE_PASSWORD: ${TEA_SVC_SPRING_DATASOURCE_PASSWORD}
      TEA_SVC_SPRING_JPA_HIBERNATE_DDL_AUTO: ${TEA_SVC_SPRING_JPA_HIBERNATE_DDL_AUTO:-validate}
      TEA_SVC_DATA_SEEDER_RUN: ${TEA_SVC_DATA_SEEDER_RUN:-false}
      LOGSTASH_HOST: logstash
      LOGSTASH_PORT: 5000
      LOGSTASH_LEVEL: INFO
      EUREKA_SERVER_URL: http://discovery-server:8761/eureka/
      EUREKA_REGISTER: true
      EUREKA_FETCH: true
      EUREKA_ENABLED: true
      MANAGEMENT_ENDPOINTS_WEB_EXPOSURE_INCLUDE: health,info,prometheus
      MANAGEMENT_METRICS_EXPORT_PROMETHEUS_ENABLED: true

    ports:
      - "8072:8084"
    networks:
      - pad-network
    depends_on:
      tea_service_postgres:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:${TEA_SVC_SERVER_PORT:-8084}/actuator/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  communication-service:
    image: smeloved/pad-communication-svc:latest
    container_name: pad-communication-service
    environment:
      SPRING_PROFILES_ACTIVE: prod
      COMMUNICATION_SVC_SERVER_PORT: ${COMMUNICATION_SVC_SERVER_PORT:-8085}
      COMMUNICATION_SVC_MONGO_URI: ${COMMUNICATION_SVC_MONGO_URI}
      COMMUNICATION_SVC_MONGO_DATABASE: ${COMMUNICATION_SVC_MONGO_DATABASE}
      COMMUNICATION_SVC_DATA_SEEDER_RUN: ${COMMUNICATION_SVC_DATA_SEEDER_RUN:-false}
      LOGSTASH_HOST: logstash
      LOGSTASH_PORT: 5000
      LOGSTASH_LEVEL: INFO
      EUREKA_SERVER_URL: http://discovery-server:8761/eureka/
      EUREKA_REGISTER: true
      EUREKA_FETCH: true
      EUREKA_ENABLED: true
    ports:
      - "8083:8085"
    networks:
      - pad-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${COMMUNICATION_SVC_SERVER_PORT:-8085}/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  lost-and-found-service:
    image: mithancik/pad-lost-and-found-service:latest
    container_name: lost-and-found-service
    restart: unless-stopped
    environment:
      NODE_ENV: production
      PORT: 3000
      DATABASE_URL: "file:/app/prisma/dev.db"
      LOGSTASH_HOST: logstash
      LOGSTASH_PORT: 5000
      LOGSTASH_LEVEL: INFO
      EUREKA_SERVER_URL: http://discovery-server:8761/eureka/
      EUREKA_REGISTER: true
      EUREKA_FETCH: true
      EUREKA_ENABLED: true
    volumes:
      - /Users/esmelov/IdeaProjects/PAD_LABS/prisma/lost-and-found:/app/prisma
    ports:
      - "3000:3000"
    networks:
      - pad-network

  lost-and-found-sqlite:
    image: keinos/sqlite3
    container_name: lost-and-found-sqlite
    stdin_open: true
    tty: true
    volumes:
      - /Users/esmelov/IdeaProjects/PAD_LABS/prisma/lost-and-found:/app/prisma
    working_dir: /app/prisma
    command: ["sqlite3", "dev.db"]
    networks:
      - pad-network

  # --- BUDGETING API ---
  budgeting-service:
    image: mithancik/pad-budgeting-service:latest
    container_name: budgeting-service
    restart: unless-stopped
    environment:
      NODE_ENV: production
      PORT: 3001
      DATABASE_URL: "file:/app/prisma/dev.db"
      LOGSTASH_HOST: logstash
      LOGSTASH_PORT: 5000
      LOGSTASH_LEVEL: INFO
      EUREKA_SERVER_URL: http://discovery-server:8761/eureka/
      EUREKA_REGISTER: true
      EUREKA_FETCH: true
      EUREKA_ENABLED: true
    volumes:
      - /Users/esmelov/IdeaProjects/PAD_LABS/prisma/budgeting:/app/prisma
    ports:
      - "3001:3001"
    networks:
      - pad-network
    depends_on:
      - lost-and-found-service

  # SQLite CLI для budgeting
  budgeting-sqlite:
    image: keinos/sqlite3
    container_name: budgeting-sqlite
    stdin_open: true
    tty: true
    volumes:
      - /Users/esmelov/IdeaProjects/PAD_LABS/prisma/budgeting:/app/prisma
    working_dir: /app/prisma
    command: ["sqlite3", "dev.db"]
    networks:
      - pad-network

  # === Fundraising stack ===
  fundraising-postgres:
    container_name: fundraising-postgres
    image: postgres:16
    environment:
      POSTGRES_DB: sharing
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_HOST: fundraising-postgres
      POSTGRES_PORT: 5432
      PGDATA: /var/lib/postgresql/data
    volumes:
      - fundraising_postgres_data:/var/lib/postgresql/data
      - ./db-init/fundraising.sql:/docker-entrypoint-initdb.d/001-fundraising.sql:ro
    ports:
      - "5434:5432"
    restart: always
    networks:
      - pad-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5

  fundraising-service:
    image: nidelcue/fund-raising-svc:latest
    container_name: fundraising-service
    environment:
      POSTGRES_DB: sharing
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_HOST: fundraising-postgres
      POSTGRES_PORT: 5432
      LOGSTASH_HOST: logstash
      LOGSTASH_PORT: 5000
      LOGSTASH_LEVEL: INFO
      EUREKA_SERVER_URL: http://discovery-server:8761/eureka/
      EUREKA_REGISTER: true
      EUREKA_FETCH: true
      EUREKA_ENABLED: true
    ports:
      - "8086:8000"
    networks:
      - pad-network
    depends_on:
      fundraising-postgres:
        condition: service_healthy
    restart: unless-stopped

  fundraising-service-2:
    image: nidelcue/fund-raising-svc:latest
    container_name: fundraising-service-2
    environment:
      POSTGRES_DB: sharing
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_HOST: fundraising-postgres
      POSTGRES_PORT: 5432
      LOGSTASH_HOST: logstash
      LOGSTASH_PORT: 5000
      LOGSTASH_LEVEL: INFO
      EUREKA_SERVER_URL: http://discovery-server:8761/eureka/
      EUREKA_REGISTER: true
      EUREKA_FETCH: true
      EUREKA_ENABLED: true
    ports:
      - "8076:8000"
    networks:
      - pad-network
    depends_on:
      fundraising-postgres:
        condition: service_healthy
    restart: unless-stopped

  # === Sharing stack ===
  sharing-postgres:
    container_name: sharing-postgres
    image: postgres:16
    environment:
      POSTGRES_DB: sharing
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_HOST: sharing-postgres
      POSTGRES_PORT: 5432
      PGDATA: /var/lib/postgresql/data add
      LOGSTASH_HOST: logstash
      LOGSTASH_PORT: 5000
      LOGSTASH_LEVEL: INFO
      EUREKA_SERVER_URL: http://discovery-server:8761/eureka/
      EUREKA_REGISTER: true
      EUREKA_FETCH: true
      EUREKA_ENABLED: true
    volumes:
      - sharing_postgres_data:/var/lib/postgresql/data
      - ./db-init/sharing.sql:/docker-entrypoint-initdb.d/001-sharing.sql:ro
    ports:
      - "5435:5432"
    restart: always
    networks:
      - pad-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5

  sharing-service:
    image: nidelcue/pad-sharing-svc:latest
    container_name: sharing-service
    environment:
      POSTGRES_DB: sharing
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_HOST: sharing-postgres
      POSTGRES_PORT: 5432
      LOGSTASH_HOST: logstash
      LOGSTASH_PORT: 5000
      LOGSTASH_LEVEL: INFO
      EUREKA_SERVER_URL: http://discovery-server:8761/eureka/
      EUREKA_REGISTER: true
      EUREKA_FETCH: true
      EUREKA_ENABLED: true
    ports:
      - "8087:8001"
    networks:
      - pad-network
    depends_on:
      sharing-postgres:
        condition: service_healthy
    restart: unless-stopped

  # === Cab booking stack ===
  cabsvc-postgres:
    image: postgres:15-alpine
    container_name: cabsvc-postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
      POSTGRES_DB: ${POSTGRES_DB:-cabsvc}
      PGDATA: /var/lib/postgresql/data
    ports:
      - "${POSTGRES_PORT:-5436}:5432"
    volumes:
      - cabsvc_postgres_data:/var/lib/postgresql/data
    networks:
      - pad-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres}"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  cab-booking-service:
    image: ${DOCKER_HUB_USERNAME:-kira9999}/cab-booking-service:${VERSION:-latest}
    container_name: cab-booking-service
    env_file:
      - .env
    environment:
      - DATABASE_URL=${DATABASE_URL:-postgresql+asyncpg://postgres:postgres@cabsvc-postgres:5432/cabsvc}
      - JWT_ALG=${JWT_ALG:-HS256}
      - JWT_SECRET=${JWT_SECRET}
      - JWT_ISS=${JWT_ISS:-faf-user-svc}
      - JWT_REQUIRED_ROLE=${JWT_REQUIRED_ROLE:-Calendar}
      - GCAL_ENABLED=${GCAL_ENABLED:-true}
      - GCAL_CALENDAR_ID=${GCAL_CALENDAR_ID:-e504816eb0c5ab7fcb30961f804f48fc6f5915a2d15b49518cd493950f30895d@group.calendar.google.com}
      - GCAL_SERVICE_ACCOUNT_JSON_PATH=${GCAL_SERVICE_ACCOUNT_JSON_PATH:-/secrets/modular-robot-454517-h5-f072d30d2647.json}
      - APP_HOST=${APP_HOST:-0.0.0.0}
      - APP_PORT=${APP_PORT:-8000}
    depends_on:
      cabsvc-postgres:
        condition: service_healthy
    ports:
      - "${APP_PORT:-8088}:8000"
    volumes:
      - ./secrets:/secrets:ro
    networks:
      - pad-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://localhost:8000/')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  mongo:
    image: mongo:6
    container_name: check-in-mongo
    restart: unless-stopped
    ports:
      - '27018:27017'
    volumes:
      - mongo-data:/data/db
    environment:
      - MONGO_INITDB_DATABASE=checkinsvc
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 40s
    networks:
      - pad-network

  check-in-service:
    image: ${DOCKER_USERNAME:-kira9999}/check-in-service:${VERSION:-latest}
    container_name: check-in-service
    restart: unless-stopped
    ports:
      - '8012:8012'
    environment:
      - MONGO_URI=mongodb://mongo:27017
      - MONGO_DB=checkinsvc
      - SERVICE_NAME=check-in-service
      - BASE_PATH=/checkinsvc/api/v1
      - LOG_LEVEL=INFO
      - TIMEZONE=UTC
    depends_on:
      mongo:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8012/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - pad-network
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'

  discovery-server:
    image: nidelcue/pad-discovery-svc:latest
    container_name: discovery-server
    ports:
      - "8761:8761"

  prometheus:
    image: prom/prometheus:v2.45.5
    container_name: prometheus
    restart: always
    ports:
      - "9092:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    depends_on:
      - gateway-service
      - discovery-server
      - logstash

  grafana:
    image: grafana/grafana-oss:8.5.2
    container_name: grafana
    restart: always
    ports:
      - "3010:3000"
    links:
      - prometheus:prometheus
    volumes:
      - ./grafana:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=password

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.3.3
    container_name: elasticsearch
    environment:
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
      - "discovery.type=single-node"
      - xpack.security.enabled=false
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data

  kibana:
    image: docker.elastic.co/kibana/kibana:8.3.3
    container_name: kibana
    ports:
      - "5601:5601"
    environment:
      ELASTICSEARCH_URL: http://elasticsearch:9200
      ELASTICSEARCH_HOSTS: '["http://elasticsearch:9200"]'
    depends_on:
      - elasticsearch

  logstash:
    image: docker.elastic.co/logstash/logstash:8.3.3
    container_name: logstash
    volumes:
      - ./logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml:ro
      - ./logstash/pipeline:/usr/share/logstash/pipeline:ro
    ports:
      - "5044:5044"
      - "5001:5001"
      - "9600:9600"
    environment:
      LS_JAVA_OPTS: "-Xmx256m -Xms256m"
    depends_on:
      - elasticsearch

  elasticsearch-exporter:
    image: quay.io/prometheuscommunity/elasticsearch-exporter:v1.7.0
    container_name: elasticsearch-exporter
    command:
      - '--es.uri=http://elasticsearch:9200'
    ports:
      - '9114:9114'
    networks:
      - pad-network
    depends_on:
      - elasticsearch
    restart: unless-stopped

  blackbox-exporter:
    image: prom/blackbox-exporter:v0.25.0
    container_name: blackbox-exporter
    ports:
      - '9115:9115'
    networks:
      - pad-network
    restart: unless-stopped


volumes:
  elasticsearch_data:
    driver: local
  usersvc_postgres_data:
    driver: local
  gateway_redis_data:
    driver: local
  teasvc_postgres_data:
    driver: local
  fundraising_postgres_data:
    driver: local
  sharing_postgres_data:
    driver: local
  cabsvc_postgres_data:
    driver: local
  mongo-data:
    driver: local
  lost_and_found_db: { }
  budgeting_db: { }


networks:
  pad-network:
    driver: bridge
